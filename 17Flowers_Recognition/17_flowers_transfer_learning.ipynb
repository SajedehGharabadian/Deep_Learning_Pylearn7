{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dJVee_HDPLHB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Flatten,Dense,Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "J3YNOdZLRdBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = '/content/drive/MyDrive/17Flowers/train'\n",
        "width = height = 224\n",
        "batch_size = 32\n",
        "\n",
        "idg = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    horizontal_flip = True,\n",
        "    brightness_range = (0.8,1.2),\n",
        "    zoom_range = 0.1,\n",
        "    shear_range = 0.3,\n",
        "    rotation_range = 10,\n",
        "    validation_split = 0.2\n",
        ")\n",
        "\n",
        "train_data = idg.flow_from_directory(\n",
        "    train_dataset_path,\n",
        "    target_size = (width,height),\n",
        "    class_mode = \"categorical\",\n",
        "    batch_size = batch_size,\n",
        "    subset = \"training\"\n",
        "\n",
        ")\n",
        "val_data = idg.flow_from_directory(\n",
        "    train_dataset_path,\n",
        "    target_size = (width,height),\n",
        "    class_mode = \"categorical\",\n",
        "    batch_size = batch_size,\n",
        "    subset = \"validation\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "g7C43c2lRUVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c0be48-af4f-40ac-8fcc-8158d9cf09d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 823 images belonging to 17 classes.\n",
            "Found 197 images belonging to 17 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "CIBHUGYjhvl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    include_top=False,\n",
        "    weights = 'imagenet',\n",
        "    input_shape=(width,height,3),\n",
        "    pooling = 'avg',\n",
        ")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "        base_model,\n",
        "        #Dropout(0.3),\n",
        "        # Flatten(),\n",
        "        # Dense(1024,activation='relu'),\n",
        "        Dense(256,activation='relu'),\n",
        "        Dense(17,activation='softmax')\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "bYRizGOUXdG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfd33b2-75ef-4e59-8a54-204bfb932b83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers[:-2]:  #freeze layer\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "COWXKenOtbst"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "WjijTVAcinGC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data,validation_data=val_data,epochs=10)"
      ],
      "metadata": {
        "id": "GdJFK58cmTLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdfd40f-b0da-4ce8-d33b-ca5262c333fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "26/26 [==============================] - 781s 30s/step - loss: 1.6099 - accuracy: 0.5565 - val_loss: 0.7207 - val_accuracy: 0.7716\n",
            "Epoch 2/10\n",
            "26/26 [==============================] - 17s 647ms/step - loss: 0.3938 - accuracy: 0.9040 - val_loss: 0.4198 - val_accuracy: 0.8528\n",
            "Epoch 3/10\n",
            "26/26 [==============================] - 17s 657ms/step - loss: 0.2276 - accuracy: 0.9502 - val_loss: 0.4303 - val_accuracy: 0.8477\n",
            "Epoch 4/10\n",
            "26/26 [==============================] - 17s 655ms/step - loss: 0.1309 - accuracy: 0.9733 - val_loss: 0.3663 - val_accuracy: 0.8274\n",
            "Epoch 5/10\n",
            "26/26 [==============================] - 17s 650ms/step - loss: 0.0945 - accuracy: 0.9793 - val_loss: 0.3449 - val_accuracy: 0.8680\n",
            "Epoch 6/10\n",
            "26/26 [==============================] - 17s 666ms/step - loss: 0.0533 - accuracy: 0.9951 - val_loss: 0.2554 - val_accuracy: 0.8832\n",
            "Epoch 7/10\n",
            "26/26 [==============================] - 17s 655ms/step - loss: 0.0512 - accuracy: 0.9915 - val_loss: 0.2437 - val_accuracy: 0.9289\n",
            "Epoch 8/10\n",
            "26/26 [==============================] - 17s 657ms/step - loss: 0.0392 - accuracy: 0.9951 - val_loss: 0.2710 - val_accuracy: 0.8934\n",
            "Epoch 9/10\n",
            "26/26 [==============================] - 17s 678ms/step - loss: 0.0368 - accuracy: 0.9964 - val_loss: 0.1898 - val_accuracy: 0.9391\n",
            "Epoch 10/10\n",
            "26/26 [==============================] - 17s 648ms/step - loss: 0.0272 - accuracy: 0.9976 - val_loss: 0.2556 - val_accuracy: 0.8985\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d6a33a27580>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_path = '/content/drive/MyDrive/17Flowers/test'\n",
        "width=height = 224\n",
        "batch_size = 32\n",
        "idg = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        ")\n",
        "\n",
        "\n",
        "test_data = idg.flow_from_directory(\n",
        "    test_dataset_path,\n",
        "    target_size = (width,height),\n",
        "    class_mode = \"categorical\",\n",
        "    batch_size = batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "izbB5iunmlMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "392c010b-59eb-4ef6-a53b-776914d3b7ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 340 images belonging to 17 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data)"
      ],
      "metadata": {
        "id": "mmqcHkG-oL9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d235f21-55e2-4918-caac-b7ed8c3893de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 [==============================] - 239s 24s/step - loss: 0.2515 - accuracy: 0.9265\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.25147515535354614, 0.9264705777168274]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('flowers_transfer_learning_1.h5')\n"
      ],
      "metadata": {
        "id": "7xfLPwAwHhYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751f78f8-39a0-450a-d566-43f538c67801"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bxvkLdN5oIm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}