{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb"
      ],
      "metadata": {
        "id": "I-btC4FsaUPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dJVee_HDPLHB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Flatten,Dense,Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "RBj6QvHhaaHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"17_Flowers_Recognition\", entity=\"gharabadiyan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "MTn1uzQaacmx",
        "outputId": "08296145-c4dc-44f9-8cdb-f1e9505328f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgharabadiyan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231028_083549-4oi3srbf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gharabadiyan/17_Flowers_Recognition/runs/4oi3srbf' target=\"_blank\">wandering-cosmos-1</a></strong> to <a href='https://wandb.ai/gharabadiyan/17_Flowers_Recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gharabadiyan/17_Flowers_Recognition' target=\"_blank\">https://wandb.ai/gharabadiyan/17_Flowers_Recognition</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gharabadiyan/17_Flowers_Recognition/runs/4oi3srbf' target=\"_blank\">https://wandb.ai/gharabadiyan/17_Flowers_Recognition/runs/4oi3srbf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gharabadiyan/17_Flowers_Recognition/runs/4oi3srbf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x79d7622cdf60>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "J3YNOdZLRdBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = '/content/drive/MyDrive/17Flowers/train'\n",
        "width = height = 224\n",
        "batch_size = 32\n",
        "\n",
        "idg = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    horizontal_flip = True,\n",
        "    brightness_range = (0.8,1.2),\n",
        "    zoom_range = 0.1,\n",
        "    shear_range = 0.3,\n",
        "    rotation_range = 10,\n",
        "    validation_split = 0.2\n",
        ")\n",
        "\n",
        "train_data = idg.flow_from_directory(\n",
        "    train_dataset_path,\n",
        "    target_size = (width,height),\n",
        "    class_mode = \"categorical\",\n",
        "    batch_size = batch_size,\n",
        "    subset = \"training\"\n",
        "\n",
        ")\n",
        "val_data = idg.flow_from_directory(\n",
        "    train_dataset_path,\n",
        "    target_size = (width,height),\n",
        "    class_mode = \"categorical\",\n",
        "    batch_size = batch_size,\n",
        "    subset = \"validation\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "g7C43c2lRUVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db21e94d-481b-498d-b76c-b4c2dd89a3cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 823 images belonging to 17 classes.\n",
            "Found 197 images belonging to 17 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "CIBHUGYjhvl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    include_top=False,\n",
        "    weights = 'imagenet',\n",
        "    input_shape=(width,height,3),\n",
        "    pooling = 'avg',\n",
        ")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "        base_model,\n",
        "        Dropout(0.3),\n",
        "        # Flatten(),\n",
        "        # Dense(1024,activation='relu'),\n",
        "        Dense(256,activation='relu'),\n",
        "        Dense(17,activation='softmax')\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "bYRizGOUXdG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac90bb4-4aa6-424e-a7b6-7a2c4b6e2d09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers[:-3]:  #freeze layer\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "COWXKenOtbst"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "WjijTVAcinGC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data,validation_data=val_data,epochs=10,callbacks=[WandbCallback()])"
      ],
      "metadata": {
        "id": "GdJFK58cmTLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe9fee7-0164-4158-c16c-16c069bb54e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "26/26 [==============================] - ETA: 0s - loss: 1.8669 - accuracy: 0.4496"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20231028_083549-4oi3srbf/files/model-best)... Done. 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26/26 [==============================] - 285s 10s/step - loss: 1.8669 - accuracy: 0.4496 - val_loss: 1.1525 - val_accuracy: 0.6650\n",
            "Epoch 2/10\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.6432 - accuracy: 0.8129"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20231028_083549-4oi3srbf/files/model-best)... Done. 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26/26 [==============================] - 36s 1s/step - loss: 0.6432 - accuracy: 0.8129 - val_loss: 0.7578 - val_accuracy: 0.7665\n",
            "Epoch 3/10\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8894"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20231028_083549-4oi3srbf/files/model-best)... Done. 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26/26 [==============================] - 36s 1s/step - loss: 0.3523 - accuracy: 0.8894 - val_loss: 0.5868 - val_accuracy: 0.8071\n",
            "Epoch 4/10\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9429"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20231028_083549-4oi3srbf/files/model-best)... Done. 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r26/26 [==============================] - 47s 2s/step - loss: 0.2227 - accuracy: 0.9429 - val_loss: 0.4340 - val_accuracy: 0.8579\n",
            "Epoch 5/10\n",
            "26/26 [==============================] - 22s 835ms/step - loss: 0.1711 - accuracy: 0.9599 - val_loss: 0.5169 - val_accuracy: 0.8426\n",
            "Epoch 6/10\n",
            "26/26 [==============================] - 22s 832ms/step - loss: 0.1537 - accuracy: 0.9526 - val_loss: 0.5179 - val_accuracy: 0.7970\n",
            "Epoch 7/10\n",
            "26/26 [==============================] - 20s 749ms/step - loss: 0.1563 - accuracy: 0.9563 - val_loss: 0.5054 - val_accuracy: 0.8173\n",
            "Epoch 8/10\n",
            "26/26 [==============================] - 19s 727ms/step - loss: 0.1203 - accuracy: 0.9635 - val_loss: 0.4479 - val_accuracy: 0.8426\n",
            "Epoch 9/10\n",
            "26/26 [==============================] - 19s 738ms/step - loss: 0.1154 - accuracy: 0.9696 - val_loss: 0.5011 - val_accuracy: 0.8426\n",
            "Epoch 10/10\n",
            "26/26 [==============================] - 21s 800ms/step - loss: 0.0969 - accuracy: 0.9708 - val_loss: 0.4673 - val_accuracy: 0.8376\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79d74036b760>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_path = '/content/drive/MyDrive/17Flowers/test'\n",
        "width=height = 224\n",
        "batch_size = 32\n",
        "idg = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        ")\n",
        "\n",
        "\n",
        "test_data = idg.flow_from_directory(\n",
        "    test_dataset_path,\n",
        "    target_size = (width,height),\n",
        "    class_mode = \"categorical\",\n",
        "    batch_size = batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "izbB5iunmlMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecf91e2-c562-4660-869e-17107dc49fe2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 340 images belonging to 17 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data)"
      ],
      "metadata": {
        "id": "mmqcHkG-oL9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd6bb5a-9087-4ccb-baea-09c2b0ffbd74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/11 [==============================] - 78s 8s/step - loss: 0.4771 - accuracy: 0.8706\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.47714418172836304, 0.8705882430076599]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('flowers_transfer_learning_1.h5')\n"
      ],
      "metadata": {
        "id": "7xfLPwAwHhYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751f78f8-39a0-450a-d566-43f538c67801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bxvkLdN5oIm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}